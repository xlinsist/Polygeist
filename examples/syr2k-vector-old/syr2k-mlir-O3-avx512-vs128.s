	.text
	.file	"LLVMDialectModule"
	.section	.rodata.cst32,"aM",@progbits,32
	.p2align	5, 0x0                          # -- Begin function main
.LCPI0_0:
	.long	0                               # 0x0
	.long	1                               # 0x1
	.long	2                               # 0x2
	.long	3                               # 0x3
	.long	4                               # 0x4
	.long	5                               # 0x5
	.long	6                               # 0x6
	.long	7                               # 0x7
.LCPI0_12:
	.long	8                               # 0x8
	.long	9                               # 0x9
	.long	10                              # 0xa
	.long	11                              # 0xb
	.long	12                              # 0xc
	.long	13                              # 0xd
	.long	14                              # 0xe
	.long	15                              # 0xf
.LCPI0_13:
	.long	16                              # 0x10
	.long	17                              # 0x11
	.long	18                              # 0x12
	.long	19                              # 0x13
	.long	20                              # 0x14
	.long	21                              # 0x15
	.long	22                              # 0x16
	.long	23                              # 0x17
.LCPI0_14:
	.long	24                              # 0x18
	.long	25                              # 0x19
	.long	26                              # 0x1a
	.long	27                              # 0x1b
	.long	28                              # 0x1c
	.long	29                              # 0x1d
	.long	30                              # 0x1e
	.long	31                              # 0x1f
.LCPI0_15:
	.long	32                              # 0x20
	.long	33                              # 0x21
	.long	34                              # 0x22
	.long	35                              # 0x23
	.long	36                              # 0x24
	.long	37                              # 0x25
	.long	38                              # 0x26
	.long	39                              # 0x27
.LCPI0_16:
	.long	40                              # 0x28
	.long	41                              # 0x29
	.long	42                              # 0x2a
	.long	43                              # 0x2b
	.long	44                              # 0x2c
	.long	45                              # 0x2d
	.long	46                              # 0x2e
	.long	47                              # 0x2f
.LCPI0_17:
	.long	48                              # 0x30
	.long	49                              # 0x31
	.long	50                              # 0x32
	.long	51                              # 0x33
	.long	52                              # 0x34
	.long	53                              # 0x35
	.long	54                              # 0x36
	.long	55                              # 0x37
.LCPI0_18:
	.long	56                              # 0x38
	.long	57                              # 0x39
	.long	58                              # 0x3a
	.long	59                              # 0x3b
	.long	60                              # 0x3c
	.long	61                              # 0x3d
	.long	62                              # 0x3e
	.long	63                              # 0x3f
.LCPI0_19:
	.long	64                              # 0x40
	.long	65                              # 0x41
	.long	66                              # 0x42
	.long	67                              # 0x43
	.long	68                              # 0x44
	.long	69                              # 0x45
	.long	70                              # 0x46
	.long	71                              # 0x47
.LCPI0_20:
	.long	72                              # 0x48
	.long	73                              # 0x49
	.long	74                              # 0x4a
	.long	75                              # 0x4b
	.long	76                              # 0x4c
	.long	77                              # 0x4d
	.long	78                              # 0x4e
	.long	79                              # 0x4f
.LCPI0_21:
	.long	80                              # 0x50
	.long	81                              # 0x51
	.long	82                              # 0x52
	.long	83                              # 0x53
	.long	84                              # 0x54
	.long	85                              # 0x55
	.long	86                              # 0x56
	.long	87                              # 0x57
.LCPI0_22:
	.long	88                              # 0x58
	.long	89                              # 0x59
	.long	90                              # 0x5a
	.long	91                              # 0x5b
	.long	92                              # 0x5c
	.long	93                              # 0x5d
	.long	94                              # 0x5e
	.long	95                              # 0x5f
.LCPI0_23:
	.long	96                              # 0x60
	.long	97                              # 0x61
	.long	98                              # 0x62
	.long	99                              # 0x63
	.long	100                             # 0x64
	.long	101                             # 0x65
	.long	102                             # 0x66
	.long	103                             # 0x67
.LCPI0_24:
	.long	104                             # 0x68
	.long	105                             # 0x69
	.long	106                             # 0x6a
	.long	107                             # 0x6b
	.long	108                             # 0x6c
	.long	109                             # 0x6d
	.long	110                             # 0x6e
	.long	111                             # 0x6f
.LCPI0_25:
	.long	112                             # 0x70
	.long	113                             # 0x71
	.long	114                             # 0x72
	.long	115                             # 0x73
	.long	116                             # 0x74
	.long	117                             # 0x75
	.long	118                             # 0x76
	.long	119                             # 0x77
.LCPI0_26:
	.long	120                             # 0x78
	.long	121                             # 0x79
	.long	122                             # 0x7a
	.long	123                             # 0x7b
	.long	124                             # 0x7c
	.long	125                             # 0x7d
	.long	126                             # 0x7e
	.long	127                             # 0x7f
	.section	.rodata.cst4,"aM",@progbits,4
	.p2align	2, 0x0
.LCPI0_1:
	.long	458129845                       # 0x1b4e81b5
.LCPI0_2:
	.long	1200                            # 0x4b0
.LCPI0_4:
	.long	2                               # 0x2
.LCPI0_5:
	.long	274877907                       # 0x10624dd3
.LCPI0_6:
	.long	1000                            # 0x3e8
.LCPI0_8:
	.long	8                               # 0x8
.LCPI0_9:
	.long	3                               # 0x3
.LCPI0_10:
	.long	16                              # 0x10
	.section	.rodata.cst8,"aM",@progbits,8
	.p2align	3, 0x0
.LCPI0_3:
	.quad	0x4092c00000000000              # double 1200
.LCPI0_7:
	.quad	0x408f400000000000              # double 1000
.LCPI0_27:
	.quad	0x3ff3333333333333              # double 1.2
.LCPI0_28:
	.quad	0x3ff8000000000000              # double 1.5
	.section	.rodata,"a",@progbits
	.p2align	6, 0x0
.LCPI0_11:
	.quad	0                               # 0x0
	.quad	8000000                         # 0x7a1200
	.quad	16000000                        # 0xf42400
	.quad	24000000                        # 0x16e3600
	.quad	32000000                        # 0x1e84800
	.quad	40000000                        # 0x2625a00
	.quad	48000000                        # 0x2dc6c00
	.quad	56000000                        # 0x3567e00
	.text
	.globl	main
	.p2align	4, 0x90
	.type	main,@function
main:                                   # @main
	.cfi_startproc
# %bb.0:
	pushq	%r15
	.cfi_def_cfa_offset 16
	pushq	%r14
	.cfi_def_cfa_offset 24
	pushq	%r12
	.cfi_def_cfa_offset 32
	pushq	%rbx
	.cfi_def_cfa_offset 40
	subq	$792, %rsp                      # imm = 0x318
	.cfi_def_cfa_offset 832
	.cfi_offset %rbx, -40
	.cfi_offset %r12, -32
	.cfi_offset %r14, -24
	.cfi_offset %r15, -16
	movl	$11520000, %edi                 # imm = 0xAFC800
	callq	malloc@PLT
	movq	%rax, %rbx
	movl	$9600000, %edi                  # imm = 0x927C00
	callq	malloc@PLT
	movq	%rax, %r14
	movl	$9600000, %edi                  # imm = 0x927C00
	callq	malloc@PLT
	movq	%rax, %r15
	xorl	%eax, %eax
	vpcmpeqd	%ymm5, %ymm5, %ymm5
	vpbroadcastd	.LCPI0_1(%rip), %ymm0   # ymm0 = [458129845,458129845,458129845,458129845,458129845,458129845,458129845,458129845]
	vpbroadcastd	.LCPI0_2(%rip), %ymm1   # ymm1 = [1200,1200,1200,1200,1200,1200,1200,1200]
	vbroadcastsd	.LCPI0_3(%rip), %zmm6   # zmm6 = [1.2E+3,1.2E+3,1.2E+3,1.2E+3,1.2E+3,1.2E+3,1.2E+3,1.2E+3]
	vpbroadcastd	.LCPI0_4(%rip), %ymm7   # ymm7 = [2,2,2,2,2,2,2,2]
	vpbroadcastd	.LCPI0_5(%rip), %ymm8   # ymm8 = [274877907,274877907,274877907,274877907,274877907,274877907,274877907,274877907]
	vpbroadcastd	.LCPI0_6(%rip), %ymm9   # ymm9 = [1000,1000,1000,1000,1000,1000,1000,1000]
	vbroadcastsd	.LCPI0_7(%rip), %zmm2   # zmm2 = [1.0E+3,1.0E+3,1.0E+3,1.0E+3,1.0E+3,1.0E+3,1.0E+3,1.0E+3]
	vpbroadcastd	.LCPI0_8(%rip), %ymm3   # ymm3 = [8,8,8,8,8,8,8,8]
	movq	%r14, %rcx
	movq	%r15, %rdx
	.p2align	4, 0x90
.LBB0_1:                                # %vector.ph
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_2 Depth 2
	vmovd	%eax, %xmm10
	vpbroadcastd	%xmm10, %ymm10
	xorl	%esi, %esi
	vmovdqa	.LCPI0_0(%rip), %ymm11          # ymm11 = [0,1,2,3,4,5,6,7]
	.p2align	4, 0x90
.LBB0_2:                                # %vector.body
                                        #   Parent Loop BB0_1 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vpmulld	%ymm10, %ymm11, %ymm12
	vpsubd	%ymm5, %ymm12, %ymm13
	vpshufd	$245, %ymm13, %ymm14            # ymm14 = ymm13[1,1,3,3,5,5,7,7]
	vpmuldq	%ymm0, %ymm14, %ymm14
	vpmuldq	%ymm0, %ymm13, %ymm15
	vpshufd	$245, %ymm15, %ymm15            # ymm15 = ymm15[1,1,3,3,5,5,7,7]
	vpblendd	$170, %ymm14, %ymm15, %ymm14    # ymm14 = ymm15[0],ymm14[1],ymm15[2],ymm14[3],ymm15[4],ymm14[5],ymm15[6],ymm14[7]
	vpsrld	$31, %ymm14, %ymm15
	vpsrad	$7, %ymm14, %ymm14
	vpaddd	%ymm7, %ymm12, %ymm12
	vpaddd	%ymm15, %ymm14, %ymm14
	vpshufd	$245, %ymm12, %ymm15            # ymm15 = ymm12[1,1,3,3,5,5,7,7]
	vpmuldq	%ymm8, %ymm15, %ymm15
	vpmuldq	%ymm8, %ymm12, %ymm4
	vpmulld	%ymm1, %ymm14, %ymm14
	vpshufd	$245, %ymm4, %ymm4              # ymm4 = ymm4[1,1,3,3,5,5,7,7]
	vpblendd	$170, %ymm15, %ymm4, %ymm4      # ymm4 = ymm4[0],ymm15[1],ymm4[2],ymm15[3],ymm4[4],ymm15[5],ymm4[6],ymm15[7]
	vpsrld	$31, %ymm4, %ymm15
	vpsubd	%ymm14, %ymm13, %ymm13
	vpsrad	$6, %ymm4, %ymm4
	vpaddd	%ymm4, %ymm15, %ymm4
	vcvtdq2pd	%ymm13, %zmm13
	vpmulld	%ymm9, %ymm4, %ymm4
	vpsubd	%ymm4, %ymm12, %ymm4
	vcvtdq2pd	%ymm4, %zmm4
	vdivpd	%zmm6, %zmm13, %zmm12
	vdivpd	%zmm2, %zmm4, %zmm4
	vmovupd	%zmm12, (%rcx,%rsi,8)
	vmovupd	%zmm4, (%rdx,%rsi,8)
	addq	$8, %rsi
	vpaddd	%ymm3, %ymm11, %ymm11
	cmpq	$1000, %rsi                     # imm = 0x3E8
	jne	.LBB0_2
# %bb.3:                                # %middle.block
                                        #   in Loop: Header=BB0_1 Depth=1
	incq	%rax
	addq	$8000, %rdx                     # imm = 0x1F40
	addq	$8000, %rcx                     # imm = 0x1F40
	cmpq	$1200, %rax                     # imm = 0x4B0
	jne	.LBB0_1
# %bb.4:                                # %.preheader.i.preheader
	xorl	%eax, %eax
	vpbroadcastd	.LCPI0_9(%rip), %ymm4   # ymm4 = [3,3,3,3,3,3,3,3]
	vmovdqa	.LCPI0_0(%rip), %ymm5           # ymm5 = [0,1,2,3,4,5,6,7]
	vpbroadcastd	.LCPI0_10(%rip), %ymm6  # ymm6 = [16,16,16,16,16,16,16,16]
	movq	%rbx, %rcx
	.p2align	4, 0x90
.LBB0_5:                                # %.preheader.i
                                        # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_6 Depth 2
	vmovd	%eax, %xmm7
	vpbroadcastd	%xmm7, %ymm7
	movl	$8, %edx
	vmovdqa	%ymm5, %ymm8
	.p2align	4, 0x90
.LBB0_6:                                # %vector.body25
                                        #   Parent Loop BB0_5 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	vpmulld	%ymm7, %ymm8, %ymm9
	vpaddd	%ymm4, %ymm9, %ymm9
	vpshufd	$245, %ymm9, %ymm10             # ymm10 = ymm9[1,1,3,3,5,5,7,7]
	vpmuldq	%ymm0, %ymm9, %ymm11
	vpmuldq	%ymm0, %ymm10, %ymm10
	vpshufd	$245, %ymm11, %ymm11            # ymm11 = ymm11[1,1,3,3,5,5,7,7]
	vpblendd	$170, %ymm10, %ymm11, %ymm10    # ymm10 = ymm11[0],ymm10[1],ymm11[2],ymm10[3],ymm11[4],ymm10[5],ymm11[6],ymm10[7]
	vpsrld	$31, %ymm10, %ymm11
	vpsrad	$7, %ymm10, %ymm10
	vpaddd	%ymm3, %ymm8, %ymm12
	vpmulld	%ymm7, %ymm12, %ymm12
	vpaddd	%ymm4, %ymm12, %ymm12
	vpaddd	%ymm11, %ymm10, %ymm10
	vpshufd	$245, %ymm12, %ymm11            # ymm11 = ymm12[1,1,3,3,5,5,7,7]
	vpmuldq	%ymm0, %ymm11, %ymm11
	vpmuldq	%ymm0, %ymm12, %ymm13
	vpmulld	%ymm1, %ymm10, %ymm10
	vpshufd	$245, %ymm13, %ymm13            # ymm13 = ymm13[1,1,3,3,5,5,7,7]
	vpblendd	$170, %ymm11, %ymm13, %ymm11    # ymm11 = ymm13[0],ymm11[1],ymm13[2],ymm11[3],ymm13[4],ymm11[5],ymm13[6],ymm11[7]
	vpsrld	$31, %ymm11, %ymm13
	vpsubd	%ymm10, %ymm9, %ymm9
	vpsrad	$7, %ymm11, %ymm10
	vpaddd	%ymm13, %ymm10, %ymm10
	vcvtdq2pd	%ymm9, %zmm9
	vpmulld	%ymm1, %ymm10, %ymm10
	vpsubd	%ymm10, %ymm12, %ymm10
	vcvtdq2pd	%ymm10, %zmm10
	vdivpd	%zmm2, %zmm9, %zmm9
	vdivpd	%zmm2, %zmm10, %zmm10
	vmovupd	%zmm9, -64(%rcx,%rdx,8)
	vmovupd	%zmm10, (%rcx,%rdx,8)
	vpaddd	%ymm6, %ymm8, %ymm8
	addq	$16, %rdx
	cmpq	$1208, %rdx                     # imm = 0x4B8
	jne	.LBB0_6
# %bb.7:                                # %middle.block21
                                        #   in Loop: Header=BB0_5 Depth=1
	incq	%rax
	addq	$9600, %rcx                     # imm = 0x2580
	cmpq	$1200, %rax                     # imm = 0x4B0
	jne	.LBB0_5
# %bb.8:                                # %init_array.exit
	vzeroupper
	callq	rtclock@PLT
	movq	polybench_t_start@GOTPCREL(%rip), %r12
	vmovq	%xmm0, (%r12)
	vpbroadcastq	%r14, %zmm0
	vmovdqa64	.LCPI0_11(%rip), %zmm1  # zmm1 = [0,8000000,16000000,24000000,32000000,40000000,48000000,56000000]
	vpaddq	%zmm1, %zmm0, %zmm0
	vpbroadcastq	%r15, %zmm2
	vpaddq	%zmm1, %zmm2, %zmm1
	movl	$1, %eax
	xorl	%ecx, %ecx
	movq	%rbx, %rdx
	vmovdqu64	%zmm0, 720(%rsp)        # 64-byte Spill
	vmovdqu64	%zmm1, 656(%rsp)        # 64-byte Spill
	vbroadcastsd	.LCPI0_28(%rip), %zmm20 # zmm20 = [1.5E+0,1.5E+0,1.5E+0,1.5E+0,1.5E+0,1.5E+0,1.5E+0,1.5E+0]
	jmp	.LBB0_10
	.p2align	4, 0x90
.LBB0_9:                                # %.loopexit.i
                                        #   in Loop: Header=BB0_10 Depth=1
	addq	$9600, %rdx                     # imm = 0x2580
	incq	%rax
	cmpq	$1200, %rcx                     # imm = 0x4B0
	je	.LBB0_20
.LBB0_10:                               # =>This Loop Header: Depth=1
                                        #     Child Loop BB0_11 Depth 2
                                        #     Child Loop BB0_16 Depth 2
                                        #       Child Loop BB0_18 Depth 3
	movq	%rcx, %rdi
	movq	%rcx, %rsi
	shrq	$7, %rsi
	incq	%rsi
	incq	%rcx
	movq	%rax, %r8
	xorl	%r9d, %r9d
	jmp	.LBB0_11
	.p2align	4, 0x90
.LBB0_13:                               #   in Loop: Header=BB0_11 Depth=2
	vmovd	%r8d, %xmm2
	vpbroadcastd	%xmm2, %ymm2
	vmovdqa	.LCPI0_19(%rip), %ymm3          # ymm3 = [64,65,66,67,68,69,70,71]
	vpcmpgtd	%zmm3, %zmm2, %k1
	kmovw	%k1, 80(%rsp)                   # 2-byte Spill
	vmovdqa	.LCPI0_20(%rip), %ymm3          # ymm3 = [72,73,74,75,76,77,78,79]
	vpcmpgtd	%zmm3, %zmm2, %k4
	kmovw	%k4, 592(%rsp)                  # 2-byte Spill
	vmovdqa	.LCPI0_21(%rip), %ymm3          # ymm3 = [80,81,82,83,84,85,86,87]
	vpcmpgtd	%zmm3, %zmm2, %k5
	kmovw	%k5, 144(%rsp)                  # 2-byte Spill
	vmovdqa	.LCPI0_22(%rip), %ymm3          # ymm3 = [88,89,90,91,92,93,94,95]
	vpcmpgtd	%zmm3, %zmm2, %k6
	kmovw	%k6, 464(%rsp)                  # 2-byte Spill
	vmovdqa	.LCPI0_23(%rip), %ymm3          # ymm3 = [96,97,98,99,100,101,102,103]
	vpcmpgtd	%zmm3, %zmm2, %k7
	kmovw	%k7, 528(%rsp)                  # 2-byte Spill
	vmovdqa	.LCPI0_24(%rip), %ymm3          # ymm3 = [104,105,106,107,108,109,110,111]
	vpcmpgtd	%zmm3, %zmm2, %k3
	kmovw	%k3, 336(%rsp)                  # 2-byte Spill
	vmovdqa	.LCPI0_25(%rip), %ymm3          # ymm3 = [112,113,114,115,116,117,118,119]
	vpcmpgtd	%zmm3, %zmm2, %k2
	kmovw	%k2, 400(%rsp)                  # 2-byte Spill
	vmovdqa	.LCPI0_26(%rip), %ymm3          # ymm3 = [120,121,122,123,124,125,126,127]
	vpcmpgtd	%zmm3, %zmm2, %k1
	kmovw	%k1, 8(%rsp)                    # 2-byte Spill
	vmovupd	960(%rdx,%r9,8), %zmm4 {%k1} {z}
	vmovupd	896(%rdx,%r9,8), %zmm5 {%k2} {z}
	vmovupd	832(%rdx,%r9,8), %zmm6 {%k3} {z}
	vmovupd	768(%rdx,%r9,8), %zmm7 {%k7} {z}
	vmovupd	704(%rdx,%r9,8), %zmm8 {%k6} {z}
	vmovupd	640(%rdx,%r9,8), %zmm9 {%k5} {z}
	vmovupd	576(%rdx,%r9,8), %zmm10 {%k4} {z}
	kmovw	80(%rsp), %k1                   # 2-byte Reload
	vmovupd	512(%rdx,%r9,8), %zmm11 {%k1} {z}
	vmovdqa	.LCPI0_18(%rip), %ymm3          # ymm3 = [56,57,58,59,60,61,62,63]
	vpcmpgtd	%zmm3, %zmm2, %k1
	kmovw	%k1, 208(%rsp)                  # 2-byte Spill
	vmovupd	448(%rdx,%r9,8), %zmm12 {%k1} {z}
	vmovdqa	.LCPI0_17(%rip), %ymm3          # ymm3 = [48,49,50,51,52,53,54,55]
	vpcmpgtd	%zmm3, %zmm2, %k6
	vmovupd	384(%rdx,%r9,8), %zmm13 {%k6} {z}
	vmovdqa	.LCPI0_16(%rip), %ymm3          # ymm3 = [40,41,42,43,44,45,46,47]
	vpcmpgtd	%zmm3, %zmm2, %k5
	vmovupd	320(%rdx,%r9,8), %zmm14 {%k5} {z}
	vmovdqa	.LCPI0_15(%rip), %ymm3          # ymm3 = [32,33,34,35,36,37,38,39]
	vpcmpgtd	%zmm3, %zmm2, %k4
	vmovupd	256(%rdx,%r9,8), %zmm15 {%k4} {z}
	vmovdqa	.LCPI0_14(%rip), %ymm3          # ymm3 = [24,25,26,27,28,29,30,31]
	vpcmpgtd	%zmm3, %zmm2, %k3
	vmovupd	192(%rdx,%r9,8), %zmm16 {%k3} {z}
	vmovdqa	.LCPI0_13(%rip), %ymm3          # ymm3 = [16,17,18,19,20,21,22,23]
	vpcmpgtd	%zmm3, %zmm2, %k1
	kmovw	%k1, 272(%rsp)                  # 2-byte Spill
	vmovupd	128(%rdx,%r9,8), %zmm17 {%k1} {z}
	vmovdqa	.LCPI0_12(%rip), %ymm3          # ymm3 = [8,9,10,11,12,13,14,15]
	vpcmpgtd	%zmm3, %zmm2, %k1
	vmovupd	64(%rdx,%r9,8), %zmm18 {%k1} {z}
	vmovdqa	.LCPI0_0(%rip), %ymm3           # ymm3 = [0,1,2,3,4,5,6,7]
	vpcmpgtd	%zmm3, %zmm2, %k7
	vmovupd	(%rdx,%r9,8), %zmm2 {%k7} {z}
	vbroadcastsd	.LCPI0_27(%rip), %zmm19 # zmm19 = [1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0]
	vmulpd	%zmm19, %zmm4, %zmm4
	kmovw	8(%rsp), %k2                    # 2-byte Reload
	vmovupd	%zmm4, 960(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm5, %zmm4
	kmovw	400(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 896(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm6, %zmm4
	kmovw	336(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 832(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm7, %zmm4
	kmovw	528(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 768(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm8, %zmm4
	kmovw	464(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 704(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm9, %zmm4
	kmovw	144(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 640(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm10, %zmm4
	kmovw	592(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 576(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm11, %zmm4
	kmovw	80(%rsp), %k2                   # 2-byte Reload
	vmovupd	%zmm4, 512(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm12, %zmm4
	kmovw	208(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 448(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm13, %zmm4
	vmovupd	%zmm4, 384(%rdx,%r9,8) {%k6}
	vmulpd	%zmm19, %zmm14, %zmm4
	vmovupd	%zmm4, 320(%rdx,%r9,8) {%k5}
	vmulpd	%zmm19, %zmm15, %zmm4
	vmovupd	%zmm4, 256(%rdx,%r9,8) {%k4}
	vmulpd	%zmm19, %zmm16, %zmm4
	vmovupd	%zmm4, 192(%rdx,%r9,8) {%k3}
	vmulpd	%zmm19, %zmm17, %zmm4
	kmovw	272(%rsp), %k2                  # 2-byte Reload
	vmovupd	%zmm4, 128(%rdx,%r9,8) {%k2}
	vmulpd	%zmm19, %zmm18, %zmm4
	vmovupd	%zmm4, 64(%rdx,%r9,8) {%k1}
	vmulpd	%zmm19, %zmm2, %zmm2
	vmovupd	%zmm2, (%rdx,%r9,8) {%k7}
	incq	%r9
	addq	$-128, %r8
	cmpq	%r9, %rsi
	je	.LBB0_15
.LBB0_11:                               #   Parent Loop BB0_10 Depth=1
                                        # =>  This Inner Loop Header: Depth=2
	cmpq	$128, %r8
	jl	.LBB0_13
# %bb.12:                               #   in Loop: Header=BB0_11 Depth=2
	vbroadcastsd	.LCPI0_27(%rip), %zmm2  # zmm2 = [1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0,1.2E+0]
	vmulpd	768(%rdx,%r9,8), %zmm2, %zmm4
	vmulpd	(%rdx,%r9,8), %zmm2, %zmm5
	vmulpd	64(%rdx,%r9,8), %zmm2, %zmm6
	vmulpd	128(%rdx,%r9,8), %zmm2, %zmm7
	vmulpd	192(%rdx,%r9,8), %zmm2, %zmm8
	vmulpd	256(%rdx,%r9,8), %zmm2, %zmm9
	vmulpd	320(%rdx,%r9,8), %zmm2, %zmm10
	vmulpd	384(%rdx,%r9,8), %zmm2, %zmm11
	vmulpd	448(%rdx,%r9,8), %zmm2, %zmm12
	vmulpd	512(%rdx,%r9,8), %zmm2, %zmm13
	vmulpd	576(%rdx,%r9,8), %zmm2, %zmm14
	vmulpd	640(%rdx,%r9,8), %zmm2, %zmm15
	vmulpd	704(%rdx,%r9,8), %zmm2, %zmm16
	vmulpd	832(%rdx,%r9,8), %zmm2, %zmm17
	vmulpd	896(%rdx,%r9,8), %zmm2, %zmm18
	vmulpd	960(%rdx,%r9,8), %zmm2, %zmm2
	vmovupd	%zmm2, 960(%rdx,%r9,8)
	vmovupd	%zmm18, 896(%rdx,%r9,8)
	vmovupd	%zmm17, 832(%rdx,%r9,8)
	vmovupd	%zmm16, 704(%rdx,%r9,8)
	vmovupd	%zmm15, 640(%rdx,%r9,8)
	vmovupd	%zmm14, 576(%rdx,%r9,8)
	vmovupd	%zmm13, 512(%rdx,%r9,8)
	vmovupd	%zmm12, 448(%rdx,%r9,8)
	vmovupd	%zmm11, 384(%rdx,%r9,8)
	vmovupd	%zmm10, 320(%rdx,%r9,8)
	vmovupd	%zmm9, 256(%rdx,%r9,8)
	vmovupd	%zmm8, 192(%rdx,%r9,8)
	vmovupd	%zmm7, 128(%rdx,%r9,8)
	vmovupd	%zmm6, 64(%rdx,%r9,8)
	vmovupd	%zmm5, (%rdx,%r9,8)
	vmovupd	%zmm4, 768(%rdx,%r9,8)
	incq	%r9
	addq	$-128, %r8
	cmpq	%r9, %rsi
	jne	.LBB0_11
.LBB0_15:                               # %.preheader.i2
                                        #   in Loop: Header=BB0_10 Depth=1
	imulq	$1000, %rdi, %rdi               # imm = 0x3E8
	xorl	%r8d, %r8d
	jmp	.LBB0_16
	.p2align	4, 0x90
.LBB0_19:                               # %.split4.us.i
                                        #   in Loop: Header=BB0_16 Depth=2
	incq	%r8
	cmpq	$1000, %r8                      # imm = 0x3E8
	je	.LBB0_9
.LBB0_16:                               #   Parent Loop BB0_10 Depth=1
                                        # =>  This Loop Header: Depth=2
                                        #       Child Loop BB0_18 Depth 3
	movq	%r8, %r9
	shlq	$7, %r9
	cmpq	$872, %r9                       # imm = 0x368
	ja	.LBB0_19
# %bb.17:                               # %.split.us.i.preheader
                                        #   in Loop: Header=BB0_16 Depth=2
	leaq	(%r8,%rdi), %r9
	vbroadcastsd	(%r14,%r9,8), %zmm21
	vbroadcastsd	(%r15,%r9,8), %zmm17
	movq	%r8, %r9
	xorl	%r10d, %r10d
	.p2align	4, 0x90
.LBB0_18:                               # %.split.us.i
                                        #   Parent Loop BB0_10 Depth=1
                                        #     Parent Loop BB0_16 Depth=2
                                        # =>    This Inner Loop Header: Depth=3
	vpbroadcastq	%r9, %zmm2
	vpsllq	$3, %zmm2, %zmm2
	vpaddq	%zmm2, %zmm0, %zmm6
	vpxor	%xmm0, %xmm0, %xmm0
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm0 {%k1}
	vmovupd	%zmm0, 80(%rsp)                 # 64-byte Spill
	vxorpd	%xmm0, %xmm0, %xmm0
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm0 {%k1}
	vmovupd	%zmm0, 272(%rsp)                # 64-byte Spill
	vxorpd	%xmm0, %xmm0, %xmm0
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm0 {%k1}
	vmovupd	%zmm0, 208(%rsp)                # 64-byte Spill
	vxorpd	%xmm0, %xmm0, %xmm0
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm0 {%k1}
	vmovupd	%zmm0, 144(%rsp)                # 64-byte Spill
	vpxord	%zmm22, %zmm22, %zmm22
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm22 {%k1}
	vpxord	%zmm18, %zmm18, %zmm18
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm18 {%k1}
	vpxord	%zmm26, %zmm26, %zmm26
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm26 {%k1}
	vpxord	%zmm28, %zmm28, %zmm28
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm28 {%k1}
	vpxord	%zmm30, %zmm30, %zmm30
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm30 {%k1}
	vxorpd	%xmm14, %xmm14, %xmm14
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm14 {%k1}
	vxorpd	%xmm5, %xmm5, %xmm5
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm5 {%k1}
	vxorpd	%xmm7, %xmm7, %xmm7
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm7 {%k1}
	vxorpd	%xmm8, %xmm8, %xmm8
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm8 {%k1}
	vxorpd	%xmm10, %xmm10, %xmm10
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm10 {%k1}
	vxorpd	%xmm11, %xmm11, %xmm11
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm11 {%k1}
	vxorpd	%xmm13, %xmm13, %xmm13
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm6), %zmm13 {%k1}
	vpaddq	%zmm2, %zmm1, %zmm2
	vpxord	%zmm23, %zmm23, %zmm23
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm23 {%k1}
	vpxord	%zmm25, %zmm25, %zmm25
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm25 {%k1}
	vpxord	%zmm27, %zmm27, %zmm27
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm27 {%k1}
	vpxord	%zmm29, %zmm29, %zmm29
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm29 {%k1}
	vpxord	%zmm31, %zmm31, %zmm31
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm31 {%k1}
	vxorpd	%xmm6, %xmm6, %xmm6
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm6 {%k1}
	vxorpd	%xmm9, %xmm9, %xmm9
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm9 {%k1}
	vxorpd	%xmm12, %xmm12, %xmm12
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm12 {%k1}
	vxorpd	%xmm15, %xmm15, %xmm15
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm15 {%k1}
	vxorpd	%xmm4, %xmm4, %xmm4
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm4 {%k1}
	vpxor	%xmm3, %xmm3, %xmm3
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm3 {%k1}
	vpxor	%xmm1, %xmm1, %xmm1
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm1 {%k1}
	vpxord	%zmm24, %zmm24, %zmm24
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm24 {%k1}
	vxorpd	%xmm0, %xmm0, %xmm0
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm0 {%k1}
	vpxord	%zmm16, %zmm16, %zmm16
	kxnorw	%k0, %k0, %k1
	vgatherqpd	(,%zmm2), %zmm16 {%k1}
	kxnorw	%k0, %k0, %k1
	vpxord	%zmm19, %zmm19, %zmm19
	vgatherqpd	(,%zmm2), %zmm19 {%k1}
	vmulpd	%zmm20, %zmm13, %zmm2
	vmulpd	%zmm20, %zmm11, %zmm11
	vmulpd	%zmm20, %zmm10, %zmm10
	vmulpd	%zmm20, %zmm8, %zmm8
	vmulpd	%zmm20, %zmm7, %zmm7
	vmulpd	%zmm20, %zmm5, %zmm13
	vmulpd	%zmm20, %zmm14, %zmm14
	vmulpd	%zmm20, %zmm30, %zmm5
	vmovupd	%zmm5, 336(%rsp)                # 64-byte Spill
	vmulpd	%zmm20, %zmm28, %zmm5
	vmovupd	%zmm5, 400(%rsp)                # 64-byte Spill
	vmulpd	%zmm20, %zmm26, %zmm5
	vmovupd	%zmm5, 464(%rsp)                # 64-byte Spill
	vmulpd	%zmm20, %zmm18, %zmm5
	vmovupd	%zmm5, 528(%rsp)                # 64-byte Spill
	vmulpd	%zmm20, %zmm22, %zmm5
	vmovupd	%zmm5, 592(%rsp)                # 64-byte Spill
	vmulpd	144(%rsp), %zmm20, %zmm5        # 64-byte Folded Reload
	vmovupd	%zmm5, 144(%rsp)                # 64-byte Spill
	vmulpd	208(%rsp), %zmm20, %zmm5        # 64-byte Folded Reload
	vmovupd	%zmm5, 208(%rsp)                # 64-byte Spill
	vmulpd	272(%rsp), %zmm20, %zmm5        # 64-byte Folded Reload
	vmovupd	%zmm5, 272(%rsp)                # 64-byte Spill
	vmulpd	80(%rsp), %zmm20, %zmm5         # 64-byte Folded Reload
	vmovupd	%zmm5, 80(%rsp)                 # 64-byte Spill
	vmulpd	%zmm20, %zmm19, %zmm18
	vmulpd	%zmm20, %zmm16, %zmm16
	vmulpd	%zmm20, %zmm0, %zmm0
	vmulpd	%zmm20, %zmm24, %zmm19
	vmulpd	%zmm20, %zmm1, %zmm22
	vmulpd	%zmm20, %zmm3, %zmm26
	vmulpd	%zmm20, %zmm4, %zmm24
	vmulpd	%zmm20, %zmm15, %zmm28
	vmulpd	%zmm20, %zmm12, %zmm30
	vmulpd	%zmm20, %zmm9, %zmm5
	vmulpd	%zmm20, %zmm6, %zmm4
	vmulpd	%zmm20, %zmm31, %zmm6
	vmulpd	%zmm20, %zmm29, %zmm9
	vmulpd	%zmm20, %zmm27, %zmm27
	vmulpd	%zmm20, %zmm25, %zmm12
	vmulpd	%zmm20, %zmm23, %zmm1
	vmulpd	%zmm2, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm1    # zmm1 = (zmm21 * zmm1) + zmm2
	vmulpd	%zmm11, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm12   # zmm12 = (zmm21 * zmm12) + zmm2
	vmulpd	%zmm10, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm27   # zmm27 = (zmm21 * zmm27) + zmm2
	vmulpd	%zmm8, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm9    # zmm9 = (zmm21 * zmm9) + zmm2
	vmulpd	%zmm7, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm6    # zmm6 = (zmm21 * zmm6) + zmm2
	vmulpd	%zmm13, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm4    # zmm4 = (zmm21 * zmm4) + zmm2
	vmulpd	%zmm14, %zmm17, %zmm2
	vfmadd213pd	%zmm2, %zmm21, %zmm5    # zmm5 = (zmm21 * zmm5) + zmm2
	vmulpd	336(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm30   # zmm30 = (zmm21 * zmm30) + zmm2
	vmulpd	400(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm28   # zmm28 = (zmm21 * zmm28) + zmm2
	vmulpd	464(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm24   # zmm24 = (zmm21 * zmm24) + zmm2
	vmulpd	528(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm26   # zmm26 = (zmm21 * zmm26) + zmm2
	vmulpd	592(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm22   # zmm22 = (zmm21 * zmm22) + zmm2
	vmulpd	144(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm19   # zmm19 = (zmm21 * zmm19) + zmm2
	vmulpd	208(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm0    # zmm0 = (zmm21 * zmm0) + zmm2
	vmulpd	272(%rsp), %zmm17, %zmm2        # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm16   # zmm16 = (zmm21 * zmm16) + zmm2
	vmulpd	80(%rsp), %zmm17, %zmm2         # 64-byte Folded Reload
	vfmadd213pd	%zmm2, %zmm21, %zmm18   # zmm18 = (zmm21 * zmm18) + zmm2
	vaddpd	960(%rdx,%r10,8), %zmm1, %zmm1
	vmovupd	%zmm1, 960(%rdx,%r10,8)
	vaddpd	832(%rdx,%r10,8), %zmm27, %zmm1
	vaddpd	896(%rdx,%r10,8), %zmm12, %zmm2
	vmovupd	%zmm2, 896(%rdx,%r10,8)
	vmovupd	%zmm1, 832(%rdx,%r10,8)
	vaddpd	768(%rdx,%r10,8), %zmm9, %zmm1
	vmovupd	%zmm1, 768(%rdx,%r10,8)
	vaddpd	704(%rdx,%r10,8), %zmm6, %zmm1
	vmovupd	%zmm1, 704(%rdx,%r10,8)
	vaddpd	576(%rdx,%r10,8), %zmm5, %zmm1
	vaddpd	640(%rdx,%r10,8), %zmm4, %zmm2
	vmovupd	%zmm2, 640(%rdx,%r10,8)
	vmovupd	%zmm1, 576(%rdx,%r10,8)
	vaddpd	512(%rdx,%r10,8), %zmm30, %zmm1
	vmovupd	%zmm1, 512(%rdx,%r10,8)
	vaddpd	448(%rdx,%r10,8), %zmm28, %zmm1
	vmovupd	%zmm1, 448(%rdx,%r10,8)
	vaddpd	320(%rdx,%r10,8), %zmm26, %zmm1
	vaddpd	384(%rdx,%r10,8), %zmm24, %zmm2
	vmovupd	%zmm2, 384(%rdx,%r10,8)
	vmovupd	%zmm1, 320(%rdx,%r10,8)
	vaddpd	256(%rdx,%r10,8), %zmm22, %zmm1
	vmovupd	%zmm1, 256(%rdx,%r10,8)
	vaddpd	192(%rdx,%r10,8), %zmm19, %zmm1
	vmovupd	%zmm1, 192(%rdx,%r10,8)
	vaddpd	64(%rdx,%r10,8), %zmm16, %zmm1
	vaddpd	128(%rdx,%r10,8), %zmm0, %zmm0
	vmovupd	%zmm0, 128(%rdx,%r10,8)
	vmovupd	%zmm1, 64(%rdx,%r10,8)
	vmovdqu64	656(%rsp), %zmm1        # 64-byte Reload
	vaddpd	(%rdx,%r10,8), %zmm18, %zmm0
	vmovupd	%zmm0, (%rdx,%r10,8)
	vmovdqu64	720(%rsp), %zmm0        # 64-byte Reload
	incq	%r10
	addq	$1000, %r9                      # imm = 0x3E8
	cmpq	%r10, %rsi
	jne	.LBB0_18
	jmp	.LBB0_19
.LBB0_20:
	vzeroupper
	callq	rtclock@PLT
	movq	polybench_t_end@GOTPCREL(%rip), %rax
	vmovq	%xmm0, (%rax)
	vsubsd	(%r12), %xmm0, %xmm0
	leaq	str0(%rip), %rdi
	movb	$1, %al
	callq	printf@PLT
	vmovsd	(%rbx), %xmm0                   # xmm0 = mem[0],zero
	vmovsd	%xmm0, 16(%rsp)                 # 8-byte Spill
	vmovsd	8(%rbx), %xmm0                  # xmm0 = mem[0],zero
	vmovsd	%xmm0, 24(%rsp)                 # 8-byte Spill
	vmovsd	16(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 32(%rsp)                 # 8-byte Spill
	vmovsd	24(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 40(%rsp)                 # 8-byte Spill
	vmovsd	32(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 48(%rsp)                 # 8-byte Spill
	vmovsd	40(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 56(%rsp)                 # 8-byte Spill
	vmovsd	48(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 64(%rsp)                 # 8-byte Spill
	vmovsd	56(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 72(%rsp)                 # 8-byte Spill
	vmovsd	64(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 8(%rsp)                  # 8-byte Spill
	vmovsd	72(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 656(%rsp)                # 8-byte Spill
	vmovsd	80(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 720(%rsp)                # 8-byte Spill
	vmovsd	88(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 336(%rsp)                # 8-byte Spill
	vmovsd	96(%rbx), %xmm0                 # xmm0 = mem[0],zero
	vmovsd	%xmm0, 400(%rsp)                # 8-byte Spill
	vmovsd	104(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 464(%rsp)                # 8-byte Spill
	vmovsd	112(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 528(%rsp)                # 8-byte Spill
	vmovsd	120(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 592(%rsp)                # 8-byte Spill
	vmovsd	128(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 144(%rsp)                # 8-byte Spill
	vmovsd	136(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 208(%rsp)                # 8-byte Spill
	vmovsd	144(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 272(%rsp)                # 8-byte Spill
	vmovsd	152(%rbx), %xmm0                # xmm0 = mem[0],zero
	vmovsd	%xmm0, 80(%rsp)                 # 8-byte Spill
	callq	printOpen@PLT
	vmovsd	16(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	24(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	32(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	40(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	48(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	56(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	64(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	72(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	8(%rsp), %xmm0                  # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	656(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	720(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	336(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	400(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	464(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	528(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	592(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	144(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	208(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	272(%rsp), %xmm0                # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printComma@PLT
	vmovsd	80(%rsp), %xmm0                 # 8-byte Reload
                                        # xmm0 = mem[0],zero
	callq	printF64@PLT
	callq	printClose@PLT
	callq	printNewline@PLT
	movq	%rbx, %rdi
	callq	free@PLT
	movq	%r14, %rdi
	callq	free@PLT
	movq	%r15, %rdi
	callq	free@PLT
	xorl	%eax, %eax
	addq	$792, %rsp                      # imm = 0x318
	.cfi_def_cfa_offset 40
	popq	%rbx
	.cfi_def_cfa_offset 32
	popq	%r12
	.cfi_def_cfa_offset 24
	popq	%r14
	.cfi_def_cfa_offset 16
	popq	%r15
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end0:
	.size	main, .Lfunc_end0-main
	.cfi_endproc
                                        # -- End function
	.globl	polybench_timer_start           # -- Begin function polybench_timer_start
	.p2align	4, 0x90
	.type	polybench_timer_start,@function
polybench_timer_start:                  # @polybench_timer_start
	.cfi_startproc
# %bb.0:
	pushq	%rax
	.cfi_def_cfa_offset 16
	callq	rtclock@PLT
	movq	polybench_t_start@GOTPCREL(%rip), %rax
	vmovsd	%xmm0, (%rax)
	popq	%rax
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end1:
	.size	polybench_timer_start, .Lfunc_end1-polybench_timer_start
	.cfi_endproc
                                        # -- End function
	.globl	polybench_timer_stop            # -- Begin function polybench_timer_stop
	.p2align	4, 0x90
	.type	polybench_timer_stop,@function
polybench_timer_stop:                   # @polybench_timer_stop
	.cfi_startproc
# %bb.0:
	pushq	%rax
	.cfi_def_cfa_offset 16
	callq	rtclock@PLT
	movq	polybench_t_end@GOTPCREL(%rip), %rax
	vmovsd	%xmm0, (%rax)
	popq	%rax
	.cfi_def_cfa_offset 8
	retq
.Lfunc_end2:
	.size	polybench_timer_stop, .Lfunc_end2-polybench_timer_stop
	.cfi_endproc
                                        # -- End function
	.globl	polybench_timer_print           # -- Begin function polybench_timer_print
	.p2align	4, 0x90
	.type	polybench_timer_print,@function
polybench_timer_print:                  # @polybench_timer_print
# %bb.0:
	movq	polybench_t_end@GOTPCREL(%rip), %rax
	vmovsd	(%rax), %xmm0                   # xmm0 = mem[0],zero
	movq	polybench_t_start@GOTPCREL(%rip), %rax
	vsubsd	(%rax), %xmm0, %xmm0
	leaq	str0(%rip), %rdi
	movb	$1, %al
	jmp	printf@PLT                      # TAILCALL
.Lfunc_end3:
	.size	polybench_timer_print, .Lfunc_end3-polybench_timer_print
                                        # -- End function
	.globl	polybench_prepare_instruments   # -- Begin function polybench_prepare_instruments
	.p2align	4, 0x90
	.type	polybench_prepare_instruments,@function
polybench_prepare_instruments:          # @polybench_prepare_instruments
# %bb.0:
	retq
.Lfunc_end4:
	.size	polybench_prepare_instruments, .Lfunc_end4-polybench_prepare_instruments
                                        # -- End function
	.type	str0,@object                    # @str0
	.section	.rodata,"a",@progbits
str0:
	.asciz	"%0.6f\n"
	.size	str0, 7

	.type	polybench_t_end,@object         # @polybench_t_end
	.bss
	.globl	polybench_t_end
	.p2align	3, 0x0
polybench_t_end:
	.zero	8
	.size	polybench_t_end, 8

	.type	polybench_t_start,@object       # @polybench_t_start
	.globl	polybench_t_start
	.p2align	3, 0x0
polybench_t_start:
	.zero	8
	.size	polybench_t_start, 8

	.section	".note.GNU-stack","",@progbits
	.addrsig
	.addrsig_sym str0
